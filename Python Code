%reset
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans

stud= pd.read_excel("C:\\Users\\madha\\OneDrive\\Documents\\Fall sem 2017\\CS_6052\\HW3-clustering\\HW3-StudentData2.xlsx")
stud.head()
stud.iloc[:,1:5]
#understanding data clusters
plt.scatter(stud['Phys'],stud['Maths'], stud['English'], stud['Music'])

#cluster iteration 1- with 3 cluster centers
Xvar=stud.iloc[:,1:5]
X1=np.random.randint(22,99,size=(3,4))

data=stud[1:]
data=np.array(data)

kmeans1= KMeans(n_clusters=3, init='random', random_state=101)
kmeans1.fit(Xvar)
#labels define the clustering for each data point
kmeans1.labels_ 
#chosen cluster centers at random
kmeans1.cluster_centers_
#([[ 55.16129032,  67.77419355,  80.03225806,  85.74193548],
#       [ 87.96153846,  88.88461538,  70.        ,  71.15384615],
#       [ 47.        ,  47.08333333,  45.16666667,  63.41666667]])
#SSE of the clustering
kmeans1.inertia_ #42879.016129032258

#cluster iteration 1- with 4 cluster centers

kmeans4= KMeans(n_clusters=4)
kmeans4.fit(data)
#labels define the clustering for each data point
kmeans4.labels_ 
#chosen cluster centers at random
kmeans4.cluster_centers_
#([[ 67.23076923,  74.15384615,  74.46153846,  81.84615385],
#       [ 40.08333333,  60.5       ,  86.83333333,  86.58333333],
#       [ 47.        ,  47.08333333,  45.16666667,  63.41666667],
#       [ 93.05263158,  92.52631579,  69.63157895,  70.57894737]])
#SSE of the clustering
kmeans4.inertia_ #35270.582995951423

kmeans5= KMeans(n_clusters=5)
kmeans5.fit(Xvar)
#labels define the clustering for each data point
kmeans5.labels_ 
#chosen cluster centers at random
kmeans5.cluster_centers_
#([[ 70.16666667,  57.83333333,  82.08333333,  71.66666667],
#       [ 63.11111111,  80.16666667,  73.16666667,  89.61111111],
#       [ 93.05263158,  92.52631579,  69.63157895,  70.57894737],
#       [ 47.        ,  47.08333333,  45.16666667,  63.41666667],
#       [ 31.375     ,  64.625     ,  84.5       ,  86.75      ]])
#SSE of the clustering
kmeans5.inertia_ #28537.417397660822


kmeans6= KMeans(n_clusters=6)
kmeans6.fit(Xvar)
#labels define the clustering for each data point
kmeans6.labels_ 
#chosen cluster centers at random
kmeans6.cluster_centers_
#([[ 40.08333333,  60.5       ,  86.83333333,  86.58333333],
#       [ 94.66666667,  91.58333333,  79.        ,  77.41666667],
#       [ 63.11111111,  80.16666667,  73.16666667,  89.61111111],
#       [ 47.        ,  47.08333333,  45.16666667,  63.41666667],
#       [ 76.5       ,  60.625     ,  77.375     ,  64.375     ],
#       [ 90.28571429,  94.14285714,  53.57142857,  58.85714286]])

#SSE of the clustering
kmeans6.inertia_ #24264.917397660822

kmeans7= KMeans(n_clusters=7)
kmeans7.fit(Xvar)
#labels define the clustering for each data point
kmeans7.labels_ 
#chosen cluster centers at random
kmeans7.cluster_centers_
#([[ 76.5       ,  60.625     ,  77.375     ,  64.375     ],
#       [ 63.11111111,  80.16666667,  73.16666667,  89.61111111],
 #      [ 40.08333333,  60.5       ,  86.83333333,  86.58333333],
  #     [ 91.25      ,  91.5       ,  61.25      ,  64.08333333],
   #    [ 49.3       ,  39.2       ,  47.1       ,  63.6       ],
    #   [ 96.14285714,  94.28571429,  84.        ,  81.71428571],
     #  [ 35.5       ,  86.5       ,  35.5       ,  62.5       ]])
#SSE of the clustering
kmeans7.inertia_ #20247.44841269841


kmeans8= KMeans(n_clusters=8)
kmeans8.fit(Xvar)
#labels define the clustering for each data point
kmeans8.labels_ 
#chosen cluster centers at random
kmeans8.cluster_centers_
#([[ 63.11111111,  80.16666667,  73.16666667,  89.61111111],
 #      [ 43.6       ,  43.        ,  56.8       ,  53.2       ],
  #     [ 90.28571429,  94.14285714,  53.57142857,  58.85714286],
   #    [ 40.08333333,  60.5       ,  86.83333333,  86.58333333],
    #   [ 76.5       ,  60.625     ,  77.375     ,  64.375     ],
     #  [ 94.66666667,  91.58333333,  79.        ,  77.41666667],
      # [ 35.5       ,  86.5       ,  35.5       ,  62.5       ],
       #[ 55.        ,  35.4       ,  37.4       ,  74.        ]])

#SSE of the clustering
kmeans8.inertia_ #17084.323412698413

#b. 
d= {'SSE': [kmeans1.inertia_, kmeans4.inertia_, kmeans5.inertia_,kmeans6.inertia_, kmeans7.inertia_,kmeans8.inertia_]
    ,'k':[3,4,5,6,7,8]}

ksse = pd.DataFrame(data=d)

from pylab import *
plot('SSE','k', data=ksse) #elbow point at 5



#c.
from __future__ import print_function
from sklearn.metrics import *
import matplotlib.cm as cm
import matplotlib.pyplot as plt
%matplotlib inline

print(__doc__)
krange=[4,5]
Xvar=stud.iloc[:,1:5]

for n_clusters in krange:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)
    
    ax1.set_xlim([-0.1, 0.7])
    ax1.set_ylim([0, len(Xvar) + (n_clusters + 1) * 1])

    clusterer = KMeans(n_clusters=n_clusters)
    cluster_labels = clusterer.fit_predict(Xvar)

    silhouette_avg = silhouette_score(Xvar, cluster_labels)
    print("For n_clusters =", n_clusters, 
          "The average silhouette_score is :", silhouette_avg)

# Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(Xvar,cluster_labels)
    y_lower = 0
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
        sample_silhouette_values[cluster_labels == i]
        ith_cluster_silhouette_values.sort()
        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i
        color = cm.spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

    ax1.text(-0.05, y_lower + 0.1 * size_cluster_i, str(i))
        # Compute the new y_lower for next plot
    y_lower = y_upper + 20000  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")
    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(Xvar.iloc[:, 0], Xvar.iloc[:, 1], marker='.', s=30,             
                lw=0, alpha=0.7,c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')
    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data " "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')
    plt.show()



#e.
df1=pd.DataFrame(np.random.randint(0,100,size=(100,4)))
kmeans4e=KMeans(n_clusters=4)
kmeans4e.fit(df1)
#labels define the clustering for each data point
kmeans4e.labels_ 
#chosen cluster centers at random
kmeans4e.cluster_centers_
#([[ 38.22580645,  70.61290323,  46.32258065,  76.06451613],
 #      [ 57.13043478,  22.13043478,  33.        ,  54.52173913],
  #     [ 60.82608696,  48.69565217,  75.2173913 ,  19.39130435],
   #    [ 39.7826087 ,  68.        ,  24.30434783,  21.26086957]])
#SSE of the clustering
kmeans4e.inertia_ #178785.15848527348

#2- Clustering 2

from scipy.cluster.hierarchy import dendrogram, linkage, fcluster, centroid
from scipy.misc import comb

stud= pd.read_excel("C:\\Users\\madha\\OneDrive\\Documents\\Fall sem 2017\\CS_6052\\HW3-clustering\\HW3-StudentData2.xlsx")
Xvar=stud.iloc[:,1:5]
dat1=stud[1:]
dat1=np.array(dat1)
l=linkage(dat1, 'single')
dn=dendrogram(l)

xlab = fcluster(l, t=4, criterion='maxclust', depth=2, R=None, monocrit=None)

#cluster index
def clustind(clustNum, labels_array):
    return np.where(labels_array == clustNum)[0]

#centroid calculation
def centroidcal(Xvar):
    numrow = Xvar.shape[0]
    sumrow1 = np.sum(Xvar[:, 0])
    sumrow2 = np.sum(Xvar[:, 1])
    sumrow3 = np.sum(Xvar[:, 2])
    sumrow4 = np.sum(Xvar[:, 3])
    return [sumrow1/numrow, sumrow2/numrow, sumrow3/numrow, sumrow4/numrow]

for clus in range(1, 4):
    print("Data in cluster "+ str(clus)+" is:")
    print(dat1[clustind(clus, xlab)])
    print("The centroid is: ") 
    print(centroidcal(dat1[clustind(clus, xlab)]))

#2a- Clustering 3
l2 = linkage(data, 'complete')
fig = plt.figure(figsize=(40, 20))
dn = dendrogram(l2)

#returns the labels of clusters from complete link dendrogram 
xlab2 = fcluster(l2, t=4, criterion='maxclust', depth=2)

for clus2 in range(1, 4):
    print("Data in cluster "+str(clus2)+" is:")
    print(data[clustind(clus2, xlab2)])
    print("The centroid is: ") 
    print(centroidcal(dat1[clustind(clus2, xlab2)]))

#Computing rand index between the clustering #stack exchange
from scipy.misc import comb
def rind(clusters, classes):

    tp_plus_fp = comb(np.bincount(clusters), 2).sum()
    tp_plus_fn = comb(np.bincount(classes), 2).sum()
    A = np.c_[(clusters, classes)]
    tp = sum(comb(np.bincount(A[A[:, 0] == i, 1]), 2).sum()
             for i in set(clusters))
    fp = tp_plus_fp - tp
    fn = tp_plus_fn - tp
    tn = comb(len(A), 2) - tp - fp - fn
    print("The value of a(same same) is: "+str(tp))
    print("The value of b(different different) is: "+str(tn))
    print("The value of c(same different) is: "+str(fp))
    print("The value of d(different same) is: "+str(fn))
    return (tp + tn) / (tp + fp + fn + tn)

print("The rand index is: "+str(rind(xlab, xlab2)))

#3Qn
print("The rand index is: "+str(rind(kmeans4.labels_, xlab)))

